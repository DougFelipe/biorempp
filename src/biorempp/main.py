import argparse
import os
import sys
from typing import Any, Dict

import pandas as pd

from biorempp.analysis.module_registry import registry
from biorempp.pipelines.input_processing import (
    run_all_processing_pipelines,
    run_biorempp_processing_pipeline,
    run_hadeg_processing_pipeline,
    run_kegg_processing_pipeline,
    run_toxcsm_processing_pipeline,
)
from biorempp.pipelines.modular_processing import ModularProcessingPipeline
from biorempp.utils.logging_config import get_logger, setup_logging

# Initialize centralized logging
setup_logging(level="INFO", console_output=True)
logger = get_logger("main")


def get_pipeline_function(pipeline_type):
    """
    Return the appropriate pipeline function based on the pipeline type.

    This function acts as a dispatcher/router for different pipeline types,
    making it easy to add new pipeline types in the future.

    Parameters
    ----------
    pipeline_type : str
        The type of pipeline to run ('all', 'biorempp', 'kegg', 'hadeg', or 'toxcsm')

    Returns
    -------
    callable
        The pipeline function to execute

    Raises
    ------
    ValueError
        If pipeline_type is not supported
    """
    pipeline_map = {
        "all": run_all_processing_pipelines,
        "biorempp": run_biorempp_processing_pipeline,
        "kegg": run_kegg_processing_pipeline,
        "hadeg": run_hadeg_processing_pipeline,
        "toxcsm": run_toxcsm_processing_pipeline,
    }

    if pipeline_type not in pipeline_map:
        available_types = list(pipeline_map.keys())
        raise ValueError(
            f"Unsupported pipeline type: '{pipeline_type}'. "
            f"Available types: {available_types}"
        )

    return pipeline_map[pipeline_type]


def run_pipeline(args):
    """
    Execute the specified pipeline with the given arguments.

    Parameters
    ----------
    args : argparse.Namespace
        Parsed command line arguments

    Returns
    -------
    str
        Path to the output file generated by the pipeline
    """
    pipeline_function = get_pipeline_function(args.pipeline_type)

    # Build the pipeline arguments dynamically based on available parameters
    pipeline_kwargs = {
        "input_path": args.input,
        "output_dir": args.output_dir,
        "add_timestamp": args.add_timestamp,
    }

    # Add optional arguments if they exist and are relevant to the pipeline
    optional_args = {
        "database": args.database,
        "biorempp_database": args.biorempp_database,
        "kegg_database": args.kegg_database,
        "hadeg_database": args.hadeg_database,
        "toxcsm_database": args.toxcsm_database,
        "output_filename": args.output_filename,
        "biorempp_output_filename": args.biorempp_output_filename,
        "kegg_output_filename": args.kegg_output_filename,
        "hadeg_output_filename": args.hadeg_output_filename,
        "toxcsm_output_filename": args.toxcsm_output_filename,
        "sep": args.sep,
    }

    # Only add arguments that are not None
    for key, value in optional_args.items():
        if value is not None:
            pipeline_kwargs[key] = value

    # Execute the pipeline
    return pipeline_function(**pipeline_kwargs)


def run_modular_pipeline(args) -> Dict[str, Any]:
    """
    Run the modular processing pipeline.

    Parameters
    ----------
    args : argparse.Namespace
        Parsed command line arguments

    Returns
    -------
    Dict[str, Any]
        Dictionary containing processing results and metadata
    """
    logger.info("Running modular processing pipeline")

    # Auto-discover modules
    registry.auto_discover_modules()

    # Load input data from file
    try:
        logger.info(f"Loading input data from: {args.input}")
        input_data = pd.read_csv(args.input, sep=";")
        logger.info(f"Loaded data shape: {input_data.shape}")
    except Exception as e:
        logger.error(f"Failed to load input data: {e}")
        raise ValueError(f"Failed to load input data: {e}")

    # Initialize pipeline
    pipeline = ModularProcessingPipeline()

    # Run processing pipeline with loaded data
    processing_results = pipeline.run_processing_pipeline(
        processor_names=args.processors, data_type="biorempp", input_data=input_data
    )

    # Prepare results summary
    results_summary = {
        "processors_run": len(args.processors),
        "successful_processors": len(processing_results),
        "processing_results": processing_results,
        "processor_details": {},
    }

    # Add processor details
    for processor_name in args.processors:
        if processor_name in processing_results:
            results_summary["processor_details"][processor_name] = {
                "rows_processed": len(processing_results[processor_name]),
                "columns": list(processing_results[processor_name].columns),
                "success": True,
            }
        else:
            results_summary["processor_details"][processor_name] = {"success": False}

    return results_summary


def print_available_modules():
    """Print all available modules to console."""
    registry.auto_discover_modules()

    processor_names = registry.list_processors()

    print("[BioRemPP] Available Modules:")
    print(f"Data Processors ({len(processor_names)}):")
    for name in processor_names:
        processor_class = registry.processors[name]
        instance = processor_class()
        print(f"  - {name}: {instance.description}")


def print_modular_results(results_summary: Dict[str, Any]):
    """
    Print modular pipeline results.

    Parameters
    ----------
    results_summary : Dict[str, Any]
        Summary of modular pipeline results
    """
    print("[BioRemPP] Modular Pipeline Results:")
    print(f"  Processors run: {results_summary['processors_run']}")
    print(f"  Successful: {results_summary['successful_processors']}")

    # Print details for each processor
    for processor_name, details in results_summary["processor_details"].items():
        if details["success"]:
            processor_name_upper = processor_name.upper()
            # Note: In the new system, we don't save files by default unless
            # explicitly requested.
            # The focus is on returning DataFrames for external use.
            print(
                f"  [{processor_name_upper}] Processing completed - "
                f"{details['rows_processed']} rows processed"
            )
            print("    Columns: {}".format(", ".join(details["columns"])))
            print("    DataFrame available for external analysis and visualization")


def setup_argument_parser():
    """
    Set up and configure the command line argument parser.

    Returns
    -------
    argparse.ArgumentParser
        Configured argument parser
    """
    parser = argparse.ArgumentParser(
        description="BioRemPP: Modular Bioinformatics Data Processing Tool"
    )

    # Required arguments
    parser.add_argument(
        "--input",
        type=str,
        required=False,  # Will be validated later based on context
        help="Path to the input file containing the data to be processed",
    )

    # Pipeline type selection
    parser.add_argument(
        "--pipeline-type",
        type=str,
        choices=["all", "biorempp", "kegg", "hadeg", "toxcsm"],
        default="all",
        help="Type of processing pipeline to run (default: all)",
    )

    # Database arguments
    parser.add_argument(
        "--database",
        type=str,
        help="Path to the main database file (overrides specific database paths)",
    )

    parser.add_argument(
        "--biorempp-database", type=str, help="Path to the BioRemPP database file"
    )

    parser.add_argument(
        "--kegg-database", type=str, help="Path to the KEGG database file"
    )

    parser.add_argument(
        "--hadeg-database", type=str, help="Path to the HAdeg database file"
    )

    parser.add_argument(
        "--toxcsm-database", type=str, help="Path to the ToxCSM database file"
    )

    # Output configuration
    parser.add_argument(
        "--output-dir",
        type=str,
        default="outputs/results_tables",
        help="Directory to save output files (default: outputs/results_tables)",
    )

    parser.add_argument(
        "--output-filename", type=str, help="Custom filename for the main output file"
    )

    parser.add_argument(
        "--biorempp-output-filename",
        type=str,
        help="Custom filename for BioRemPP output",
    )

    parser.add_argument(
        "--kegg-output-filename", type=str, help="Custom filename for KEGG output"
    )

    parser.add_argument(
        "--hadeg-output-filename", type=str, help="Custom filename for HAdeg output"
    )

    parser.add_argument(
        "--toxcsm-output-filename", type=str, help="Custom filename for ToxCSM output"
    )

    # File format options
    parser.add_argument(
        "--sep",
        type=str,
        default=";",
        help="Separator character for output files (default: ;)",
    )

    # Timestamp options
    timestamp_group = parser.add_mutually_exclusive_group()
    timestamp_group.add_argument(
        "--add-timestamp",
        action="store_true",
        default=False,
        help="Add timestamp to output filenames",
    )

    timestamp_group.add_argument(
        "--no-timestamp",
        action="store_false",
        dest="add_timestamp",
        help="Do not add timestamp to output filenames (default)",
    )

    # Modular processing options
    parser.add_argument(
        "--enable-modular",
        action="store_true",
        help="Enable modular processing pipeline (returns DataFrames for external use)",
    )

    parser.add_argument(
        "--processors",
        nargs="+",
        help="List of data processors to run in modular mode "
        "(e.g., gene_pathway_analyzer compound_class_analyzer)",
    )

    # Information options
    parser.add_argument(
        "--list-modules",
        action="store_true",
        help="List all available processing modules and exit",
    )

    return parser


def main():
    """
    Main entry point for the BioRemPP application.

    This function orchestrates the entire data processing workflow,
    focusing exclusively on data processing. Analysis results are
    returned as DataFrames for external use.
    """
    parser = setup_argument_parser()
    args = parser.parse_args()

    try:
        logger.info("Starting BioRemPP processing pipeline")

        # Handle information requests
        if args.list_modules:
            print_available_modules()
            return

        # Validate input file is provided for processing operations
        if not args.input:
            logger.error("Input file is required for processing operations")
            print(
                "[ERROR] Input file is required. Use --input to specify the file path."
            )
            sys.exit(1)

        # Validate input file exists
        if not os.path.exists(args.input):
            logger.error(f"Input file not found: {args.input}")
            print(f"[ERROR] Input file not found: {args.input}")
            sys.exit(1)

        # Resolve absolute path for input
        input_path = os.path.abspath(args.input)
        logger.info(f"Resolved input path: {input_path}")

        # Handle modular processing
        if args.enable_modular:
            if not args.processors:
                logger.error("Modular processing requires --processors to be specified")
                print(
                    "[ERROR] When using --enable-modular, you must specify --processors"
                )
                sys.exit(1)

            # Run modular pipeline and return results
            results_summary = run_modular_pipeline(args)
            print_modular_results(results_summary)

            # Return the results for potential external use
            return results_summary["processing_results"]

        # Handle traditional pipeline processing
        logger.info(f"Running {args.pipeline_type} pipeline")
        pipeline_output = run_pipeline(args)

        if isinstance(pipeline_output, dict):
            # Handle multiple outputs (e.g., from 'all' pipeline)
            logger.info("All pipelines completed successfully")
            print("[BioRemPP] All processing pipelines completed successfully:")
            for pipeline_name, output_path in pipeline_output.items():
                logger.info(f"{pipeline_name.upper()} output saved to: {output_path}")
                print(f"  [{pipeline_name.upper()}] Output: {output_path}")
        else:
            # Handle single output
            logger.info(
                f"Pipeline completed successfully. Output saved to: {pipeline_output}"
            )
            print(f"[BioRemPP] Output processed and saved to: {pipeline_output}")

        return {"pipeline_output": pipeline_output}

    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
        print("\n[BioRemPP] Process interrupted by user")
        sys.exit(130)
    except Exception as e:
        logger.error(f"An error occurred: {e}")
        print(f"[ERROR] {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
